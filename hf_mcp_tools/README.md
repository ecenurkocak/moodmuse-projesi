# Hugging Face MCP Tool & Server

Bu proje, Hugging Face modelleri ile etkileÅŸim kurmak iÃ§in hem komut satÄ±rÄ± aracÄ± hem de MCP (Model Context Protocol) server iÃ§erir.

## ğŸ“ Dosya YapÄ±sÄ±

```
capstone/
â”œâ”€â”€ hf_mcp_tool.py          # Komut satÄ±rÄ± aracÄ±
â”œâ”€â”€ hf_mcp_server.py        # MCP Server
â”œâ”€â”€ mcp_server_config.json  # Server konfigÃ¼rasyonu
â”œâ”€â”€ test_mcp_client.py      # Test client
â”œâ”€â”€ start_server.bat        # Windows server baÅŸlatma
â”œâ”€â”€ start_server.sh         # Linux/Mac server baÅŸlatma
â”œâ”€â”€ test_server.bat         # Windows test script
â”œâ”€â”€ test_server.sh          # Linux/Mac test script
â”œâ”€â”€ requirements.txt        # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â””â”€â”€ README.md              # Bu dosya
```

## ğŸš€ Kurulum

```bash
pip install -r requirements.txt
```

**Not:** Ä°lk kurulumda `transformers`, `torch` ve diÄŸer kÃ¼tÃ¼phaneler otomatik olarak yÃ¼klenecektir. Bu iÅŸlem biraz zaman alabilir.

## ğŸ”§ KullanÄ±m

### 1. Token Ayarlama (Ä°steÄŸe baÄŸlÄ±)
EÄŸer Ã¶zel modeller kullanacaksan, Hugging Face token'Ä±nÄ±zÄ± ayarlayÄ±n:

```bash
# Windows
set HF_TOKEN=hf_your_token_here

# Linux/Mac
export HF_TOKEN=hf_your_token_here
```

### 2. Komut SatÄ±rÄ± AracÄ±

#### PopÃ¼ler modelleri listele:
```bash
python hf_mcp_tool.py --list
```

#### Model bilgisi al:
```bash
python hf_mcp_tool.py --info gpt2
```

#### Inference yap:
```bash
python hf_mcp_tool.py --infer gpt2 "Hello world"
```

### 3. MCP Server

#### Server'Ä± baÅŸlat:
```bash
# Windows
start_server.bat

# Linux/Mac
chmod +x start_server.sh
./start_server.sh
```

#### Server'Ä± test et:
```bash
# Windows
test_server.bat

# Linux/Mac
chmod +x test_server.sh
./test_server.sh
```

#### Manuel test:
```bash
# Health check
curl http://localhost:3000/health

# List models
curl http://localhost:3000/models

# Inference (API)
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{"model_id": "gpt2", "inputs": "Hello world"}'

# Inference (Local - requires transformers/torch)
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{"model_id": "gpt2", "inputs": "Hello world", "use_local": true}'
```

## ğŸ“‹ API Endpoints

- `GET /health` - Server durumu
- `GET /models` - Mevcut modeller
- `POST /inference` - Model inference (API veya Local)

### Inference Parameters:
- `model_id` (required): Model adÄ±
- `inputs` (required): Girdi metni
- `use_local` (optional): Yerel Ã§alÄ±ÅŸtÄ±rma iÃ§in `true` (transformers/torch gerekli)

## ğŸ§ª Test Ã–rnekleri

```bash
# Text generation
python hf_mcp_tool.py --infer gpt2 "The future of AI is"

# Sentiment analysis
python hf_mcp_tool.py --infer distilbert-base-uncased-finetuned-sst-2-english "I love this movie!"

# Translation
python hf_mcp_tool.py --infer t5-small "translate English to French: Hello world"
```

## ğŸ”§ KonfigÃ¼rasyon

`mcp_server_config.json` dosyasÄ±nda server ayarlarÄ±nÄ± deÄŸiÅŸtirebilirsin:

```json
{
  "mcpServers": {
    "huggingface-mcp": {
      "command": "python",
      "args": ["hf_mcp_server.py"],
      "env": {
        "HF_TOKEN": "${HF_TOKEN}",
        "MCP_SERVER_PORT": "3000"
      }
    }
  }
}
```

## ğŸ“Š Desteklenen Model TÃ¼rleri

- Text Generation (GPT-2, DialoGPT)
- Text Classification (BERT, DistilBERT)
- Translation (T5, mT5)
- Summarization (BART, T5)
- Question Answering (BERT variants)

## ğŸ› Sorun Giderme

1. **Server baÅŸlamÄ±yor**: Python ve requests kÃ¼tÃ¼phanesinin yÃ¼klÃ¼ olduÄŸundan emin ol
2. **Inference hatasÄ±**: HF_TOKEN'Ä±n doÄŸru ayarlandÄ±ÄŸÄ±ndan emin ol
3. **Port Ã§akÄ±ÅŸmasÄ±**: `MCP_SERVER_PORT` environment variable'Ä±nÄ± deÄŸiÅŸtir 